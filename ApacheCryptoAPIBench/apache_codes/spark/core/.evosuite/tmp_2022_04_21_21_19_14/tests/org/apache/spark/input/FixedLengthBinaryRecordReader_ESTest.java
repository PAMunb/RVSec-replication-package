/*
 * This file was automatically generated by EvoSuite
 * Thu Apr 21 23:22:22 GMT 2022
 */

package org.apache.spark.input;

import org.junit.Test;
import static org.junit.Assert.*;
import static org.evosuite.shaded.org.mockito.Mockito.*;
import static org.evosuite.runtime.EvoAssertions.*;
import org.apache.hadoop.mapreduce.InputSplit;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
import org.apache.spark.input.FixedLengthBinaryRecordReader;
import org.evosuite.runtime.EvoRunner;
import org.evosuite.runtime.EvoRunnerParameters;
import org.evosuite.runtime.ViolatedAssumptionAnswer;
import org.junit.runner.RunWith;

@RunWith(EvoRunner.class) @EvoRunnerParameters(mockJVMNonDeterminism = true, useVFS = true, useVNET = true, resetStaticState = true, separateClassLoader = true) 
public class FixedLengthBinaryRecordReader_ESTest extends FixedLengthBinaryRecordReader_ESTest_scaffolding {

  @Test(timeout = 4000)
  public void test0()  throws Throwable  {
      FixedLengthBinaryRecordReader fixedLengthBinaryRecordReader0 = new FixedLengthBinaryRecordReader();
      // Undeclared exception!
      try { 
        fixedLengthBinaryRecordReader0.initialize((InputSplit) null, (TaskAttemptContext) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.spark.input.FixedLengthBinaryRecordReader", e);
      }
  }

  @Test(timeout = 4000)
  public void test1()  throws Throwable  {
      FixedLengthBinaryRecordReader fixedLengthBinaryRecordReader0 = new FixedLengthBinaryRecordReader();
      InputSplit inputSplit0 = mock(InputSplit.class, new ViolatedAssumptionAnswer());
      TaskAttemptContext taskAttemptContext0 = mock(TaskAttemptContext.class, new ViolatedAssumptionAnswer());
      // Undeclared exception!
      try { 
        fixedLengthBinaryRecordReader0.initialize(inputSplit0, taskAttemptContext0);
        fail("Expecting exception: ClassCastException");
      
      } catch(ClassCastException e) {
         //
         // org.apache.hadoop.mapreduce.InputSplit$MockitoMock$414148539 cannot be cast to org.apache.hadoop.mapreduce.lib.input.FileSplit
         //
         verifyException("org.apache.spark.input.FixedLengthBinaryRecordReader", e);
      }
  }

  @Test(timeout = 4000)
  public void test2()  throws Throwable  {
      FixedLengthBinaryRecordReader fixedLengthBinaryRecordReader0 = new FixedLengthBinaryRecordReader();
      fixedLengthBinaryRecordReader0.close();
  }

  @Test(timeout = 4000)
  public void test3()  throws Throwable  {
      FixedLengthBinaryRecordReader fixedLengthBinaryRecordReader0 = new FixedLengthBinaryRecordReader();
      fixedLengthBinaryRecordReader0.getCurrentKey();
  }

  @Test(timeout = 4000)
  public void test4()  throws Throwable  {
      FixedLengthBinaryRecordReader fixedLengthBinaryRecordReader0 = new FixedLengthBinaryRecordReader();
      // Undeclared exception!
      try { 
        fixedLengthBinaryRecordReader0.nextKeyValue();
        fail("Expecting exception: ArithmeticException");
      
      } catch(ArithmeticException e) {
         //
         // / by zero
         //
         verifyException("org.apache.spark.input.FixedLengthBinaryRecordReader", e);
      }
  }

  @Test(timeout = 4000)
  public void test5()  throws Throwable  {
      FixedLengthBinaryRecordReader fixedLengthBinaryRecordReader0 = new FixedLengthBinaryRecordReader();
      fixedLengthBinaryRecordReader0.getCurrentValue();
  }

  @Test(timeout = 4000)
  public void test6()  throws Throwable  {
      FixedLengthBinaryRecordReader fixedLengthBinaryRecordReader0 = new FixedLengthBinaryRecordReader();
      fixedLengthBinaryRecordReader0.getProgress();
  }
}
