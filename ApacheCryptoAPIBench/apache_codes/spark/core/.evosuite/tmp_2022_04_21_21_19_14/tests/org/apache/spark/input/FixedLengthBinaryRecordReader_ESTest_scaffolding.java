/**
 * Scaffolding file used to store all the setups needed to run 
 * tests automatically generated by EvoSuite
 * Thu Apr 21 23:22:22 GMT 2022
 */

package org.apache.spark.input;

import org.evosuite.runtime.annotation.EvoSuiteClassExclude;
import org.junit.BeforeClass;
import org.junit.Before;
import org.junit.After;
import org.junit.AfterClass;
import org.evosuite.runtime.sandbox.Sandbox;
import org.evosuite.runtime.sandbox.Sandbox.SandboxMode;

import static org.evosuite.shaded.org.mockito.Mockito.*;
@EvoSuiteClassExclude
public class FixedLengthBinaryRecordReader_ESTest_scaffolding {

  @org.junit.Rule
  public org.evosuite.runtime.vnet.NonFunctionalRequirementRule nfr = new org.evosuite.runtime.vnet.NonFunctionalRequirementRule();

  private static final java.util.Properties defaultProperties = (java.util.Properties) java.lang.System.getProperties().clone(); 

  private org.evosuite.runtime.thread.ThreadStopper threadStopper =  new org.evosuite.runtime.thread.ThreadStopper (org.evosuite.runtime.thread.KillSwitchHandler.getInstance(), 3000);


  @BeforeClass
  public static void initEvoSuiteFramework() { 
    org.evosuite.runtime.RuntimeSettings.className = "org.apache.spark.input.FixedLengthBinaryRecordReader"; 
    org.evosuite.runtime.GuiSupport.initialize(); 
    org.evosuite.runtime.RuntimeSettings.maxNumberOfThreads = 100; 
    org.evosuite.runtime.RuntimeSettings.maxNumberOfIterationsPerLoop = 10000; 
    org.evosuite.runtime.RuntimeSettings.mockSystemIn = true; 
    org.evosuite.runtime.RuntimeSettings.sandboxMode = org.evosuite.runtime.sandbox.Sandbox.SandboxMode.RECOMMENDED; 
    org.evosuite.runtime.sandbox.Sandbox.initializeSecurityManagerForSUT(); 
    org.evosuite.runtime.classhandling.JDKClassResetter.init();
    setSystemProperties();
    initializeClasses();
    org.evosuite.runtime.Runtime.getInstance().resetRuntime(); 
    try { initMocksToAvoidTimeoutsInTheTests(); } catch(ClassNotFoundException e) {} 
  } 

  @AfterClass
  public static void clearEvoSuiteFramework(){ 
    Sandbox.resetDefaultSecurityManager(); 
    java.lang.System.setProperties((java.util.Properties) defaultProperties.clone()); 
  } 

  @Before
  public void initTestCase(){ 
    threadStopper.storeCurrentThreads();
    threadStopper.startRecordingTime();
    org.evosuite.runtime.jvm.ShutdownHookHandler.getInstance().initHandler(); 
    org.evosuite.runtime.sandbox.Sandbox.goingToExecuteSUTCode(); 
    setSystemProperties(); 
    org.evosuite.runtime.GuiSupport.setHeadless(); 
    org.evosuite.runtime.Runtime.getInstance().resetRuntime(); 
    org.evosuite.runtime.agent.InstrumentingAgent.activate(); 
  } 

  @After
  public void doneWithTestCase(){ 
    threadStopper.killAndJoinClientThreads();
    org.evosuite.runtime.jvm.ShutdownHookHandler.getInstance().safeExecuteAddedHooks(); 
    org.evosuite.runtime.classhandling.JDKClassResetter.reset(); 
    resetClasses(); 
    org.evosuite.runtime.sandbox.Sandbox.doneWithExecutingSUTCode(); 
    org.evosuite.runtime.agent.InstrumentingAgent.deactivate(); 
    org.evosuite.runtime.GuiSupport.restoreHeadlessMode(); 
  } 

  public static void setSystemProperties() {
 
    java.lang.System.setProperties((java.util.Properties) defaultProperties.clone()); 
  }

  private static void initializeClasses() {
    org.evosuite.runtime.classhandling.ClassStateSupport.initializeClasses(FixedLengthBinaryRecordReader_ESTest_scaffolding.class.getClassLoader() ,
      "org.apache.hadoop.mapred.lib.CombineFileSplit",
      "org.apache.hadoop.util.Progressable",
      "org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl$DummyReporter",
      "org.apache.hadoop.fs.FileSystem",
      "org.apache.commons.collections.map.UnmodifiableMap",
      "scala.reflect.ScalaSignature",
      "org.apache.hadoop.mapreduce.lib.input.TaggedInputSplit",
      "org.apache.hadoop.mapreduce.StatusReporter",
      "org.apache.hadoop.io.Writable",
      "org.apache.hadoop.mapreduce.lib.db.DataDrivenDBInputFormat$DataDrivenDBInputSplit",
      "org.apache.hadoop.io.LongWritable",
      "org.apache.hadoop.mapreduce.OutputFormat",
      "org.apache.hadoop.mapreduce.lib.input.FileSplit",
      "org.apache.hadoop.fs.Seekable",
      "org.apache.hadoop.io.BytesWritable$Comparator",
      "org.apache.commons.collections.map.AbstractMapDecorator",
      "org.apache.hadoop.classification.InterfaceAudience$Private",
      "org.apache.hadoop.fs.CanUnbuffer",
      "org.apache.hadoop.mapred.FileSplit",
      "org.apache.hadoop.mapreduce.MRJobConfig",
      "org.apache.hadoop.mapreduce.Mapper$Context",
      "org.apache.hadoop.conf.Configured",
      "org.apache.hadoop.conf.Configuration",
      "org.apache.hadoop.conf.Configuration$1",
      "org.apache.hadoop.fs.Path",
      "org.apache.commons.collections.MapIterator",
      "org.apache.hadoop.classification.InterfaceAudience$Public",
      "org.apache.hadoop.conf.Configurable",
      "org.apache.hadoop.security.Credentials",
      "org.apache.hadoop.conf.Configuration$NegativeCacheSentinel",
      "org.apache.hadoop.classification.InterfaceStability$Stable",
      "org.apache.hadoop.fs.FSDataInputStream",
      "org.apache.hadoop.mapreduce.TaskInputOutputContext",
      "org.apache.hadoop.mapreduce.lib.input.CombineFileSplit",
      "org.apache.hadoop.mapred.MultiFileSplit",
      "org.apache.hadoop.mapreduce.TaskAttemptID",
      "org.apache.hadoop.mapreduce.task.MapContextImpl",
      "org.apache.hadoop.conf.Configuration$DeprecationContext",
      "org.apache.hadoop.mapred.InputSplitWithLocationInfo",
      "org.apache.hadoop.mapreduce.Reducer$Context",
      "org.apache.hadoop.fs.ByteBufferReadable",
      "org.apache.commons.collections.IterableMap",
      "org.apache.hadoop.mapreduce.Mapper",
      "org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl",
      "org.apache.hadoop.fs.CanSetReadahead",
      "org.apache.hadoop.fs.ChecksumFileSystem",
      "org.apache.hadoop.fs.HasFileDescriptor",
      "org.apache.hadoop.mapreduce.lib.db.DataDrivenDBInputFormat",
      "org.apache.hadoop.mapreduce.task.annotation.Checkpointable",
      "org.apache.hadoop.mapreduce.ReduceContext",
      "org.apache.hadoop.mapreduce.MapContext",
      "org.apache.hadoop.io.WritableComparable",
      "org.apache.hadoop.conf.Configuration$DeprecationDelta",
      "org.apache.hadoop.io.BinaryComparable",
      "org.apache.hadoop.mapreduce.task.ReduceContextImpl",
      "org.apache.hadoop.classification.InterfaceStability$Evolving",
      "org.apache.hadoop.io.WritableComparator",
      "org.apache.hadoop.mapreduce.task.JobContextImpl",
      "org.apache.hadoop.mapred.SplitLocationInfo",
      "org.apache.hadoop.conf.Configuration$DeprecatedKeyInfo",
      "org.apache.hadoop.fs.FilterFileSystem",
      "org.apache.hadoop.mapreduce.Reducer",
      "org.apache.hadoop.fs.PositionedReadable",
      "org.apache.hadoop.fs.CanSetDropBehind",
      "org.apache.hadoop.io.BytesWritable",
      "org.apache.hadoop.mapreduce.lib.join.CompositeInputSplit",
      "org.apache.hadoop.mapred.ID",
      "org.apache.hadoop.mapreduce.lib.db.DBInputFormat$DBInputSplit",
      "org.apache.hadoop.mapreduce.JobID",
      "org.apache.hadoop.classification.InterfaceStability$Unstable",
      "org.apache.hadoop.mapreduce.TaskAttemptContext",
      "org.apache.hadoop.mapreduce.InputSplit",
      "org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl",
      "org.apache.hadoop.mapreduce.InputFormat",
      "org.apache.hadoop.mapreduce.Counter",
      "org.apache.hadoop.conf.Configuration$IntegerRanges",
      "org.apache.spark.input.FixedLengthBinaryRecordReader",
      "org.apache.hadoop.fs.HasEnhancedByteBufferAccess",
      "org.apache.hadoop.mapreduce.lib.db.DBInputFormat",
      "org.apache.hadoop.mapreduce.RecordReader",
      "org.apache.hadoop.mapreduce.Partitioner",
      "org.apache.hadoop.mapred.JobConf",
      "org.apache.hadoop.io.RawComparator",
      "org.apache.hadoop.io.LongWritable$Comparator",
      "org.apache.hadoop.mapreduce.OutputCommitter",
      "org.apache.hadoop.mapreduce.JobContext",
      "org.apache.hadoop.mapred.InputSplit",
      "com.google.common.base.Preconditions",
      "org.apache.hadoop.mapreduce.ID",
      "org.apache.commons.collections.Unmodifiable",
      "org.apache.spark.input.FixedLengthBinaryInputFormat$",
      "org.apache.hadoop.fs.LocalFileSystem"
    );
  } 
  private static void initMocksToAvoidTimeoutsInTheTests() throws ClassNotFoundException { 
    mock(Class.forName("org.apache.hadoop.mapreduce.InputSplit", false, FixedLengthBinaryRecordReader_ESTest_scaffolding.class.getClassLoader()));
    mock(Class.forName("org.apache.hadoop.mapreduce.TaskAttemptContext", false, FixedLengthBinaryRecordReader_ESTest_scaffolding.class.getClassLoader()));
  }

  private static void resetClasses() {
    org.evosuite.runtime.classhandling.ClassResetter.getInstance().setClassLoader(FixedLengthBinaryRecordReader_ESTest_scaffolding.class.getClassLoader()); 

    org.evosuite.runtime.classhandling.ClassStateSupport.resetClasses(
      "org.apache.hadoop.mapreduce.RecordReader",
      "org.apache.spark.input.FixedLengthBinaryRecordReader",
      "org.apache.spark.input.FixedLengthBinaryInputFormat$",
      "org.apache.hadoop.io.WritableComparator",
      "org.apache.hadoop.io.LongWritable$Comparator",
      "org.apache.hadoop.conf.Configuration$DeprecationDelta",
      "com.google.common.base.Preconditions",
      "org.apache.hadoop.conf.Configuration$DeprecationContext",
      "org.apache.hadoop.conf.Configuration$DeprecatedKeyInfo",
      "org.apache.commons.collections.map.AbstractMapDecorator",
      "org.apache.commons.collections.map.UnmodifiableMap",
      "org.apache.hadoop.conf.Configuration",
      "org.apache.hadoop.io.LongWritable",
      "org.apache.hadoop.mapreduce.InputSplit"
    );
  }
}
