/*
 * This file was automatically generated by EvoSuite
 * Thu Apr 21 23:21:28 GMT 2022
 */

package org.apache.spark.ui.storage;

import org.junit.Test;
import static org.junit.Assert.*;
import static org.evosuite.shaded.org.mockito.Mockito.*;
import static org.evosuite.runtime.EvoAssertions.*;
import org.apache.spark.status.api.v1.RDDPartitionInfo;
import org.apache.spark.ui.storage.BlockDataSource;
import org.evosuite.runtime.EvoRunner;
import org.evosuite.runtime.EvoRunnerParameters;
import org.evosuite.runtime.ViolatedAssumptionAnswer;
import org.junit.runner.RunWith;
import scala.collection.Seq;
import scala.collection.immutable.Range;
import scala.collection.mutable.Queue;

@RunWith(EvoRunner.class) @EvoRunnerParameters(mockJVMNonDeterminism = true, useVFS = true, useVNET = true, resetStaticState = true, separateClassLoader = true) 
public class BlockDataSource_ESTest extends BlockDataSource_ESTest_scaffolding {

  @Test(timeout = 4000)
  public void test0()  throws Throwable  {
      Queue<Range> queue0 = (Queue<Range>) mock(Queue.class, new ViolatedAssumptionAnswer());
      doReturn((Object) null).when(queue0).sorted(any(scala.math.Ordering.class));
      Seq<RDDPartitionInfo> seq0 = (Seq<RDDPartitionInfo>) mock(Seq.class, new ViolatedAssumptionAnswer());
      doReturn(queue0).when(seq0).map(any(scala.Function1.class) , any(scala.collection.generic.CanBuildFrom.class));
      BlockDataSource blockDataSource0 = new BlockDataSource(seq0, 1000, "Storage Level", true);
      Seq<String> seq1 = (Seq<String>) mock(Seq.class, new ViolatedAssumptionAnswer());
      doReturn((String) null).when(seq1).mkString(anyString());
      RDDPartitionInfo rDDPartitionInfo0 = mock(RDDPartitionInfo.class, new ViolatedAssumptionAnswer());
      doReturn("Storage Level").when(rDDPartitionInfo0).blockName();
      doReturn((long)1000).when(rDDPartitionInfo0).diskUsed();
      doReturn(seq1).when(rDDPartitionInfo0).executors();
      doReturn(0L).when(rDDPartitionInfo0).memoryUsed();
      doReturn("R !Z|$n.<ec5{+,Pl").when(rDDPartitionInfo0).storageLevel();
      blockDataSource0.org$apache$spark$ui$storage$BlockDataSource$$blockRow(rDDPartitionInfo0);
  }

  @Test(timeout = 4000)
  public void test1()  throws Throwable  {
      Queue<Range> queue0 = (Queue<Range>) mock(Queue.class, new ViolatedAssumptionAnswer());
      doReturn((Object) null).when(queue0).sorted(any(scala.math.Ordering.class));
      Seq<RDDPartitionInfo> seq0 = (Seq<RDDPartitionInfo>) mock(Seq.class, new ViolatedAssumptionAnswer());
      doReturn(queue0).when(seq0).map(any(scala.Function1.class) , any(scala.collection.generic.CanBuildFrom.class));
      BlockDataSource blockDataSource0 = new BlockDataSource(seq0, 1000, "Storage Level", true);
      Seq<String> seq1 = (Seq<String>) mock(Seq.class, new ViolatedAssumptionAnswer());
      doReturn((String) null).when(seq1).mkString(anyString());
      RDDPartitionInfo rDDPartitionInfo0 = mock(RDDPartitionInfo.class, new ViolatedAssumptionAnswer());
      doReturn((String) null).when(rDDPartitionInfo0).blockName();
      doReturn(0L).when(rDDPartitionInfo0).diskUsed();
      doReturn(seq1).when(rDDPartitionInfo0).executors();
      doReturn(1L).when(rDDPartitionInfo0).memoryUsed();
      doReturn((String) null).when(rDDPartitionInfo0).storageLevel();
      blockDataSource0.org$apache$spark$ui$storage$BlockDataSource$$blockRow(rDDPartitionInfo0);
  }

  @Test(timeout = 4000)
  public void test2()  throws Throwable  {
      Queue<Range> queue0 = (Queue<Range>) mock(Queue.class, new ViolatedAssumptionAnswer());
      doReturn((Object) null).when(queue0).sorted(any(scala.math.Ordering.class));
      Seq<RDDPartitionInfo> seq0 = (Seq<RDDPartitionInfo>) mock(Seq.class, new ViolatedAssumptionAnswer());
      doReturn(queue0).when(seq0).map(any(scala.Function1.class) , any(scala.collection.generic.CanBuildFrom.class));
      BlockDataSource blockDataSource0 = new BlockDataSource(seq0, 1000, "Storage Level", true);
      Seq<String> seq1 = (Seq<String>) mock(Seq.class, new ViolatedAssumptionAnswer());
      doReturn((String) null).when(seq1).mkString(anyString());
      RDDPartitionInfo rDDPartitionInfo0 = mock(RDDPartitionInfo.class, new ViolatedAssumptionAnswer());
      doReturn("720/'1v8").when(rDDPartitionInfo0).blockName();
      doReturn((-2092L)).when(rDDPartitionInfo0).diskUsed();
      doReturn(seq1).when(rDDPartitionInfo0).executors();
      doReturn((-2092L)).when(rDDPartitionInfo0).memoryUsed();
      doReturn(",:P>").when(rDDPartitionInfo0).storageLevel();
      blockDataSource0.org$apache$spark$ui$storage$BlockDataSource$$blockRow(rDDPartitionInfo0);
  }

  @Test(timeout = 4000)
  public void test3()  throws Throwable  {
      Seq<RDDPartitionInfo> seq0 = (Seq<RDDPartitionInfo>) mock(Seq.class, new ViolatedAssumptionAnswer());
      doReturn((Object) null).when(seq0).map(any(scala.Function1.class) , any(scala.collection.generic.CanBuildFrom.class));
      BlockDataSource blockDataSource0 = null;
      try {
        blockDataSource0 = new BlockDataSource(seq0, 1000, "", false);
        fail("Expecting exception: IllegalArgumentException");
      
      } catch(IllegalArgumentException e) {
         //
         // Unknown column: 
         //
         verifyException("org.apache.spark.ui.storage.BlockDataSource", e);
      }
  }

  @Test(timeout = 4000)
  public void test4()  throws Throwable  {
      Seq<RDDPartitionInfo> seq0 = (Seq<RDDPartitionInfo>) mock(Seq.class, new ViolatedAssumptionAnswer());
      doReturn((Object) null).when(seq0).map(any(scala.Function1.class) , any(scala.collection.generic.CanBuildFrom.class));
      BlockDataSource blockDataSource0 = null;
      try {
        blockDataSource0 = new BlockDataSource(seq0, 1, "Size in Memory", true);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.spark.ui.storage.BlockDataSource", e);
      }
  }

  @Test(timeout = 4000)
  public void test5()  throws Throwable  {
      Seq<RDDPartitionInfo> seq0 = (Seq<RDDPartitionInfo>) mock(Seq.class, new ViolatedAssumptionAnswer());
      doReturn((Object) null).when(seq0).map(any(scala.Function1.class) , any(scala.collection.generic.CanBuildFrom.class));
      BlockDataSource blockDataSource0 = null;
      try {
        blockDataSource0 = new BlockDataSource(seq0, 4929, "Block Name", false);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.spark.ui.storage.BlockDataSource", e);
      }
  }

  @Test(timeout = 4000)
  public void test6()  throws Throwable  {
      Range range0 = mock(Range.class, new ViolatedAssumptionAnswer());
      doReturn((Object) null).when(range0).sorted(any(scala.math.Ordering.class));
      Seq<RDDPartitionInfo> seq0 = (Seq<RDDPartitionInfo>) mock(Seq.class, new ViolatedAssumptionAnswer());
      doReturn(range0).when(seq0).map(any(scala.Function1.class) , any(scala.collection.generic.CanBuildFrom.class));
      BlockDataSource blockDataSource0 = new BlockDataSource(seq0, 98, "Executors", true);
  }

  @Test(timeout = 4000)
  public void test7()  throws Throwable  {
      Range range0 = mock(Range.class, new ViolatedAssumptionAnswer());
      doReturn((Object) null).when(range0).sorted(any(scala.math.Ordering.class));
      Seq<RDDPartitionInfo> seq0 = (Seq<RDDPartitionInfo>) mock(Seq.class, new ViolatedAssumptionAnswer());
      doReturn(range0).when(seq0).map(any(scala.Function1.class) , any(scala.collection.generic.CanBuildFrom.class));
      BlockDataSource blockDataSource0 = new BlockDataSource(seq0, 1759, "Storage Level", false);
      RDDPartitionInfo rDDPartitionInfo0 = mock(RDDPartitionInfo.class, new ViolatedAssumptionAnswer());
      doReturn((String) null).when(rDDPartitionInfo0).blockName();
      doReturn(0L).when(rDDPartitionInfo0).diskUsed();
      doReturn((Seq) null).when(rDDPartitionInfo0).executors();
      doReturn(0L).when(rDDPartitionInfo0).memoryUsed();
      doReturn((String) null).when(rDDPartitionInfo0).storageLevel();
      // Undeclared exception!
      try { 
        blockDataSource0.org$apache$spark$ui$storage$BlockDataSource$$blockRow(rDDPartitionInfo0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.spark.ui.storage.BlockDataSource", e);
      }
  }

  @Test(timeout = 4000)
  public void test8()  throws Throwable  {
      Range range0 = mock(Range.class, new ViolatedAssumptionAnswer());
      doReturn((Object) null).when(range0).sorted(any(scala.math.Ordering.class));
      Seq<RDDPartitionInfo> seq0 = (Seq<RDDPartitionInfo>) mock(Seq.class, new ViolatedAssumptionAnswer());
      doReturn(range0).when(seq0).map(any(scala.Function1.class) , any(scala.collection.generic.CanBuildFrom.class));
      BlockDataSource blockDataSource0 = new BlockDataSource(seq0, 1000, "Size on Disk", true);
      // Undeclared exception!
      try { 
        blockDataSource0.sliceData(1000, 0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.spark.ui.storage.BlockDataSource", e);
      }
  }

  @Test(timeout = 4000)
  public void test9()  throws Throwable  {
      Queue<Range> queue0 = (Queue<Range>) mock(Queue.class, new ViolatedAssumptionAnswer());
      doReturn((Object) null).when(queue0).sorted(any(scala.math.Ordering.class));
      Seq<RDDPartitionInfo> seq0 = (Seq<RDDPartitionInfo>) mock(Seq.class, new ViolatedAssumptionAnswer());
      doReturn(queue0).when(seq0).map(any(scala.Function1.class) , any(scala.collection.generic.CanBuildFrom.class));
      BlockDataSource blockDataSource0 = new BlockDataSource(seq0, 839, "Storage Level", true);
      // Undeclared exception!
      try { 
        blockDataSource0.dataSize();
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.spark.ui.storage.BlockDataSource", e);
      }
  }
}
