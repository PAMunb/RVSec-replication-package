/*
 * This file was automatically generated by EvoSuite
 * Thu Apr 21 22:00:24 GMT 2022
 */

package org.apache.spark.rdd;

import org.junit.Test;
import static org.junit.Assert.*;
import static org.evosuite.shaded.org.mockito.Mockito.*;
import static org.evosuite.runtime.EvoAssertions.*;
import org.apache.spark.Partition;
import org.apache.spark.SparkContext;
import org.apache.spark.rdd.DefaultPartitionCoalescer;
import org.apache.spark.rdd.PartitionGroup;
import org.apache.spark.rdd.RDD;
import org.apache.spark.scheduler.TaskLocation;
import org.evosuite.runtime.EvoRunner;
import org.evosuite.runtime.EvoRunnerParameters;
import org.evosuite.runtime.ViolatedAssumptionAnswer;
import org.junit.runner.RunWith;
import scala.Option;
import scala.collection.Seq;
import scala.collection.mutable.ArrayBuffer;
import scala.collection.mutable.HashMap;

@RunWith(EvoRunner.class) @EvoRunnerParameters(mockJVMNonDeterminism = true, useVFS = true, useVNET = true, resetStaticState = true, separateClassLoader = true) 
public class DefaultPartitionCoalescer_ESTest extends DefaultPartitionCoalescer_ESTest_scaffolding {

  @Test(timeout = 4000)
  public void test00()  throws Throwable  {
      DefaultPartitionCoalescer defaultPartitionCoalescer0 = new DefaultPartitionCoalescer(0.05);
      defaultPartitionCoalescer0.noLocality_$eq(false);
      defaultPartitionCoalescer0.noLocality();
  }

  @Test(timeout = 4000)
  public void test01()  throws Throwable  {
      DefaultPartitionCoalescer defaultPartitionCoalescer0 = new DefaultPartitionCoalescer(2973.69596019822);
      Partition[] partitionArray0 = new Partition[1];
      Partition partition0 = mock(Partition.class, new ViolatedAssumptionAnswer());
      doReturn("H1%;Y&YJBu5K8%", "H1%;Y&YJBu5K8%").when(partition0).toString();
      doReturn(1371).when(partition0).index();
      partitionArray0[0] = partition0;
      ArrayBuffer<PartitionGroup> arrayBuffer0 = defaultPartitionCoalescer0.groupArr();
      Seq<TaskLocation> seq0 = (Seq<TaskLocation>) mock(Seq.class, new ViolatedAssumptionAnswer());
      doReturn(arrayBuffer0).when(seq0).map(any(scala.Function1.class) , any(scala.collection.generic.CanBuildFrom.class));
      SparkContext sparkContext0 = mock(SparkContext.class, new ViolatedAssumptionAnswer());
      doReturn(seq0).when(sparkContext0).getPreferredLocs(any(org.apache.spark.rdd.RDD.class) , anyInt());
      RDD<PartitionGroup> rDD0 = (RDD<PartitionGroup>) mock(RDD.class, new ViolatedAssumptionAnswer());
      doReturn(sparkContext0).when(rDD0).context();
      doReturn((Object) partitionArray0, (Object) partitionArray0, (Object) partitionArray0).when(rDD0).partitions();
      defaultPartitionCoalescer0.coalesce(1371, rDD0);
      defaultPartitionCoalescer0.groupArr();
  }

  @Test(timeout = 4000)
  public void test02()  throws Throwable  {
      DefaultPartitionCoalescer defaultPartitionCoalescer0 = new DefaultPartitionCoalescer(2973.69596019822);
      Partition partition0 = mock(Partition.class, new ViolatedAssumptionAnswer());
      doReturn(1371).when(partition0).index();
      ArrayBuffer<PartitionGroup> arrayBuffer0 = defaultPartitionCoalescer0.groupArr();
      Seq<TaskLocation> seq0 = (Seq<TaskLocation>) mock(Seq.class, new ViolatedAssumptionAnswer());
      doReturn(arrayBuffer0).when(seq0).map(any(scala.Function1.class) , any(scala.collection.generic.CanBuildFrom.class));
      SparkContext sparkContext0 = mock(SparkContext.class, new ViolatedAssumptionAnswer());
      doReturn(seq0).when(sparkContext0).getPreferredLocs(any(org.apache.spark.rdd.RDD.class) , anyInt());
      RDD<PartitionGroup> rDD0 = (RDD<PartitionGroup>) mock(RDD.class, new ViolatedAssumptionAnswer());
      doReturn(sparkContext0).when(rDD0).context();
      defaultPartitionCoalescer0.currPrefLocs(partition0, rDD0);
  }

  @Test(timeout = 4000)
  public void test03()  throws Throwable  {
      DefaultPartitionCoalescer defaultPartitionCoalescer0 = new DefaultPartitionCoalescer(2973.69596019822);
      defaultPartitionCoalescer0.balanceSlack();
  }

  @Test(timeout = 4000)
  public void test04()  throws Throwable  {
      DefaultPartitionCoalescer defaultPartitionCoalescer0 = new DefaultPartitionCoalescer((-3062));
      defaultPartitionCoalescer0.balanceSlack();
  }

  @Test(timeout = 4000)
  public void test05()  throws Throwable  {
      DefaultPartitionCoalescer defaultPartitionCoalescer0 = new DefaultPartitionCoalescer(9.227227120071749);
      // Undeclared exception!
      try { 
        defaultPartitionCoalescer0.throwBalls(164, (RDD<?>) null, 164, (DefaultPartitionCoalescer.PartitionLocations) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.spark.rdd.DefaultPartitionCoalescer", e);
      }
  }

  @Test(timeout = 4000)
  public void test06()  throws Throwable  {
      DefaultPartitionCoalescer defaultPartitionCoalescer0 = new DefaultPartitionCoalescer(8102);
      // Undeclared exception!
      try { 
        defaultPartitionCoalescer0.setupGroups(128, (DefaultPartitionCoalescer.PartitionLocations) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.spark.rdd.DefaultPartitionCoalescer", e);
      }
  }

  @Test(timeout = 4000)
  public void test07()  throws Throwable  {
      DefaultPartitionCoalescer defaultPartitionCoalescer0 = new DefaultPartitionCoalescer((-3062));
      // Undeclared exception!
      try { 
        defaultPartitionCoalescer0.pickBin((Partition) null, (RDD<?>) null, 0.0, (DefaultPartitionCoalescer.PartitionLocations) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.spark.rdd.DefaultPartitionCoalescer", e);
      }
  }

  @Test(timeout = 4000)
  public void test08()  throws Throwable  {
      DefaultPartitionCoalescer defaultPartitionCoalescer0 = new DefaultPartitionCoalescer(2973.69596019822);
      Partition[] partitionArray0 = new Partition[1];
      Partition partition0 = mock(Partition.class, new ViolatedAssumptionAnswer());
      doReturn(1371).when(partition0).index();
      HashMap<PartitionGroup, PartitionGroup> hashMap0 = (HashMap<PartitionGroup, PartitionGroup>) mock(HashMap.class, new ViolatedAssumptionAnswer());
      Seq<TaskLocation> seq0 = (Seq<TaskLocation>) mock(Seq.class, new ViolatedAssumptionAnswer());
      doReturn(hashMap0).when(seq0).map(any(scala.Function1.class) , any(scala.collection.generic.CanBuildFrom.class));
      SparkContext sparkContext0 = mock(SparkContext.class, new ViolatedAssumptionAnswer());
      doReturn(seq0).when(sparkContext0).getPreferredLocs(any(org.apache.spark.rdd.RDD.class) , anyInt());
      RDD<PartitionGroup> rDD0 = (RDD<PartitionGroup>) mock(RDD.class, new ViolatedAssumptionAnswer());
      doReturn(sparkContext0).when(rDD0).context();
      doReturn(partitionArray0).when(rDD0).partitions();
      // Undeclared exception!
      try { 
        defaultPartitionCoalescer0.pickBin(partition0, rDD0, (-3062), (DefaultPartitionCoalescer.PartitionLocations) null);
        fail("Expecting exception: ClassCastException");
      
      } catch(ClassCastException e) {
         //
         // scala.collection.mutable.HashMap$MockitoMock$1015253124 cannot be cast to scala.collection.Seq
         //
         verifyException("org.apache.spark.rdd.DefaultPartitionCoalescer", e);
      }
  }

  @Test(timeout = 4000)
  public void test09()  throws Throwable  {
      DefaultPartitionCoalescer defaultPartitionCoalescer0 = new DefaultPartitionCoalescer((-3079));
      // Undeclared exception!
      try { 
        defaultPartitionCoalescer0.compare((PartitionGroup) null, (PartitionGroup) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.spark.rdd.DefaultPartitionCoalescer", e);
      }
  }

  @Test(timeout = 4000)
  public void test10()  throws Throwable  {
      DefaultPartitionCoalescer defaultPartitionCoalescer0 = new DefaultPartitionCoalescer(0.05);
      // Undeclared exception!
      try { 
        defaultPartitionCoalescer0.coalesce(1246, (RDD<?>) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.spark.rdd.DefaultPartitionCoalescer$PartitionLocations", e);
      }
  }

  @Test(timeout = 4000)
  public void test11()  throws Throwable  {
      DefaultPartitionCoalescer defaultPartitionCoalescer0 = new DefaultPartitionCoalescer(0.0);
      Partition[] partitionArray0 = new Partition[0];
      Partition[] partitionArray1 = new Partition[4];
      RDD<PartitionGroup> rDD0 = (RDD<PartitionGroup>) mock(RDD.class, new ViolatedAssumptionAnswer());
      doReturn((Object) partitionArray0, (Object) partitionArray0, (Object) partitionArray1).when(rDD0).partitions();
      // Undeclared exception!
      try { 
        defaultPartitionCoalescer0.coalesce(1246, rDD0);
        fail("Expecting exception: IndexOutOfBoundsException");
      
      } catch(IndexOutOfBoundsException e) {
         //
         // java.lang.Integer@0000000044
         //
         verifyException("scala.collection.mutable.ResizableArray$class", e);
      }
  }

  @Test(timeout = 4000)
  public void test12()  throws Throwable  {
      DefaultPartitionCoalescer defaultPartitionCoalescer0 = new DefaultPartitionCoalescer(2973.69596019822);
      Partition[] partitionArray0 = new Partition[1];
      Partition partition0 = mock(Partition.class, new ViolatedAssumptionAnswer());
      doReturn((-3062)).when(partition0).index();
      partitionArray0[0] = partition0;
      HashMap<PartitionGroup, PartitionGroup> hashMap0 = (HashMap<PartitionGroup, PartitionGroup>) mock(HashMap.class, new ViolatedAssumptionAnswer());
      Seq<TaskLocation> seq0 = (Seq<TaskLocation>) mock(Seq.class, new ViolatedAssumptionAnswer());
      doReturn(hashMap0).when(seq0).map(any(scala.Function1.class) , any(scala.collection.generic.CanBuildFrom.class));
      SparkContext sparkContext0 = mock(SparkContext.class, new ViolatedAssumptionAnswer());
      doReturn(seq0).when(sparkContext0).getPreferredLocs(any(org.apache.spark.rdd.RDD.class) , anyInt());
      RDD<PartitionGroup> rDD0 = (RDD<PartitionGroup>) mock(RDD.class, new ViolatedAssumptionAnswer());
      doReturn(sparkContext0).when(rDD0).context();
      doReturn(partitionArray0).when(rDD0).partitions();
      // Undeclared exception!
      try { 
        defaultPartitionCoalescer0.coalesce((-3062), rDD0);
        fail("Expecting exception: ClassCastException");
      
      } catch(ClassCastException e) {
         //
         // scala.collection.mutable.HashMap$MockitoMock$1015253124 cannot be cast to scala.collection.Seq
         //
         verifyException("org.apache.spark.rdd.DefaultPartitionCoalescer", e);
      }
  }

  @Test(timeout = 4000)
  public void test13()  throws Throwable  {
      DefaultPartitionCoalescer defaultPartitionCoalescer0 = new DefaultPartitionCoalescer(2973.69596019822);
      Partition[] partitionArray0 = new Partition[1];
      Partition partition0 = mock(Partition.class, new ViolatedAssumptionAnswer());
      doReturn("Gr8m", "Gr8m").when(partition0).toString();
      doReturn(1371).when(partition0).index();
      partitionArray0[0] = partition0;
      ArrayBuffer<PartitionGroup> arrayBuffer0 = defaultPartitionCoalescer0.groupArr();
      Seq<TaskLocation> seq0 = (Seq<TaskLocation>) mock(Seq.class, new ViolatedAssumptionAnswer());
      doReturn(arrayBuffer0).when(seq0).map(any(scala.Function1.class) , any(scala.collection.generic.CanBuildFrom.class));
      SparkContext sparkContext0 = mock(SparkContext.class, new ViolatedAssumptionAnswer());
      doReturn(seq0).when(sparkContext0).getPreferredLocs(any(org.apache.spark.rdd.RDD.class) , anyInt());
      Partition[] partitionArray1 = new Partition[9];
      RDD<PartitionGroup> rDD0 = (RDD<PartitionGroup>) mock(RDD.class, new ViolatedAssumptionAnswer());
      doReturn(sparkContext0).when(rDD0).context();
      doReturn((Object) partitionArray0, (Object) partitionArray0, (Object) partitionArray0, (Object) partitionArray1, (Object) partitionArray0).when(rDD0).partitions();
      // Undeclared exception!
      try { 
        defaultPartitionCoalescer0.coalesce(1, rDD0);
        fail("Expecting exception: ArrayIndexOutOfBoundsException");
      
      } catch(ArrayIndexOutOfBoundsException e) {
         //
         // 1
         //
         verifyException("org.apache.spark.rdd.DefaultPartitionCoalescer$$anonfun$throwBalls$1$$anonfun$apply$mcVI$sp$2", e);
      }
  }

  @Test(timeout = 4000)
  public void test14()  throws Throwable  {
      DefaultPartitionCoalescer defaultPartitionCoalescer0 = new DefaultPartitionCoalescer(2973.69596019822);
      Partition[] partitionArray0 = new Partition[1];
      Partition partition0 = mock(Partition.class, new ViolatedAssumptionAnswer());
      doReturn("H1%;Y&YJBu5K8%", "H1%;Y&YJBu5K8%").when(partition0).toString();
      doReturn(1371).when(partition0).index();
      defaultPartitionCoalescer0.noLocality_$eq(false);
      partitionArray0[0] = partition0;
      ArrayBuffer<PartitionGroup> arrayBuffer0 = defaultPartitionCoalescer0.groupArr();
      Seq<TaskLocation> seq0 = (Seq<TaskLocation>) mock(Seq.class, new ViolatedAssumptionAnswer());
      doReturn(arrayBuffer0).when(seq0).map(any(scala.Function1.class) , any(scala.collection.generic.CanBuildFrom.class));
      SparkContext sparkContext0 = mock(SparkContext.class, new ViolatedAssumptionAnswer());
      doReturn(seq0).when(sparkContext0).getPreferredLocs(any(org.apache.spark.rdd.RDD.class) , anyInt());
      RDD<PartitionGroup> rDD0 = (RDD<PartitionGroup>) mock(RDD.class, new ViolatedAssumptionAnswer());
      doReturn(sparkContext0).when(rDD0).context();
      doReturn((Object) partitionArray0, (Object) partitionArray0, (Object) partitionArray0, (Object) null).when(rDD0).partitions();
      // Undeclared exception!
      try { 
        defaultPartitionCoalescer0.coalesce(0, rDD0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.spark.rdd.DefaultPartitionCoalescer", e);
      }
  }

  @Test(timeout = 4000)
  public void test15()  throws Throwable  {
      DefaultPartitionCoalescer defaultPartitionCoalescer0 = new DefaultPartitionCoalescer(134.8200944127386);
      defaultPartitionCoalescer0.throwBalls((-2987), (RDD<?>) null, 134.8200944127386, (DefaultPartitionCoalescer.PartitionLocations) null);
      assertEquals(134.8200944127386, defaultPartitionCoalescer0.balanceSlack(), 0.01);
  }

  @Test(timeout = 4000)
  public void test16()  throws Throwable  {
      DefaultPartitionCoalescer defaultPartitionCoalescer0 = new DefaultPartitionCoalescer(2973.69596019822);
      Partition[] partitionArray0 = new Partition[18];
      Partition partition0 = mock(Partition.class, new ViolatedAssumptionAnswer());
      doReturn((-3062)).when(partition0).index();
      ArrayBuffer<PartitionGroup> arrayBuffer0 = defaultPartitionCoalescer0.groupArr();
      Seq<TaskLocation> seq0 = (Seq<TaskLocation>) mock(Seq.class, new ViolatedAssumptionAnswer());
      doReturn(arrayBuffer0).when(seq0).map(any(scala.Function1.class) , any(scala.collection.generic.CanBuildFrom.class));
      SparkContext sparkContext0 = mock(SparkContext.class, new ViolatedAssumptionAnswer());
      doReturn(seq0).when(sparkContext0).getPreferredLocs(any(org.apache.spark.rdd.RDD.class) , anyInt());
      RDD<PartitionGroup> rDD0 = (RDD<PartitionGroup>) mock(RDD.class, new ViolatedAssumptionAnswer());
      doReturn(sparkContext0).when(rDD0).context();
      doReturn(partitionArray0).when(rDD0).partitions();
      // Undeclared exception!
      try { 
        defaultPartitionCoalescer0.pickBin(partition0, rDD0, (-3062), (DefaultPartitionCoalescer.PartitionLocations) null);
        fail("Expecting exception: ArithmeticException");
      
      } catch(ArithmeticException e) {
         //
         // / by zero
         //
         verifyException("org.evosuite.runtime.Random", e);
      }
  }

  @Test(timeout = 4000)
  public void test17()  throws Throwable  {
      DefaultPartitionCoalescer defaultPartitionCoalescer0 = new DefaultPartitionCoalescer((-1816.65557));
      Partition[] partitionArray0 = new Partition[0];
      RDD<PartitionGroup> rDD0 = (RDD<PartitionGroup>) mock(RDD.class, new ViolatedAssumptionAnswer());
      doReturn(partitionArray0).when(rDD0).partitions();
      DefaultPartitionCoalescer.PartitionLocations defaultPartitionCoalescer_PartitionLocations0 = defaultPartitionCoalescer0.new PartitionLocations(rDD0);
      defaultPartitionCoalescer0.setupGroups(402, defaultPartitionCoalescer_PartitionLocations0);
      assertTrue(defaultPartitionCoalescer0.noLocality());
  }

  @Test(timeout = 4000)
  public void test18()  throws Throwable  {
      DefaultPartitionCoalescer defaultPartitionCoalescer0 = new DefaultPartitionCoalescer((-740.9));
      boolean boolean0 = defaultPartitionCoalescer0.noLocality();
      assertTrue(boolean0);
  }

  @Test(timeout = 4000)
  public void test19()  throws Throwable  {
      DefaultPartitionCoalescer defaultPartitionCoalescer0 = new DefaultPartitionCoalescer(2973.69596019822);
      Partition[] partitionArray0 = new Partition[1];
      Partition partition0 = mock(Partition.class, new ViolatedAssumptionAnswer());
      doReturn((String) null).when(partition0).toString();
      doReturn(1371).when(partition0).index();
      partitionArray0[0] = partition0;
      ArrayBuffer<PartitionGroup> arrayBuffer0 = defaultPartitionCoalescer0.groupArr();
      Seq<TaskLocation> seq0 = (Seq<TaskLocation>) mock(Seq.class, new ViolatedAssumptionAnswer());
      doReturn(arrayBuffer0).when(seq0).map(any(scala.Function1.class) , any(scala.collection.generic.CanBuildFrom.class));
      SparkContext sparkContext0 = mock(SparkContext.class, new ViolatedAssumptionAnswer());
      doReturn(seq0).when(sparkContext0).getPreferredLocs(any(org.apache.spark.rdd.RDD.class) , anyInt());
      RDD<PartitionGroup> rDD0 = (RDD<PartitionGroup>) mock(RDD.class, new ViolatedAssumptionAnswer());
      doReturn(sparkContext0).when(rDD0).context();
      doReturn(partitionArray0).when(rDD0).partitions();
      DefaultPartitionCoalescer.PartitionLocations defaultPartitionCoalescer_PartitionLocations0 = defaultPartitionCoalescer0.new PartitionLocations(rDD0);
      defaultPartitionCoalescer_PartitionLocations0.partsWithLocs();
      assertTrue(defaultPartitionCoalescer0.noLocality());
  }

  @Test(timeout = 4000)
  public void test20()  throws Throwable  {
      DefaultPartitionCoalescer defaultPartitionCoalescer0 = new DefaultPartitionCoalescer(3.141592653589793);
      defaultPartitionCoalescer0.groupHash();
      assertTrue(defaultPartitionCoalescer0.noLocality());
  }

  @Test(timeout = 4000)
  public void test21()  throws Throwable  {
      DefaultPartitionCoalescer defaultPartitionCoalescer0 = new DefaultPartitionCoalescer(0.0);
      defaultPartitionCoalescer0.rnd();
      assertTrue(defaultPartitionCoalescer0.noLocality());
  }

  @Test(timeout = 4000)
  public void test22()  throws Throwable  {
      DefaultPartitionCoalescer defaultPartitionCoalescer0 = new DefaultPartitionCoalescer(0.0);
      Partition[] partitionArray0 = new Partition[0];
      RDD<PartitionGroup> rDD0 = (RDD<PartitionGroup>) mock(RDD.class, new ViolatedAssumptionAnswer());
      doReturn(partitionArray0).when(rDD0).partitions();
      DefaultPartitionCoalescer.PartitionLocations defaultPartitionCoalescer_PartitionLocations0 = defaultPartitionCoalescer0.new PartitionLocations(rDD0);
      RDD<PartitionGroup> rDD1 = (RDD<PartitionGroup>) mock(RDD.class, new ViolatedAssumptionAnswer());
      doReturn((Partition[]) null).when(rDD1).partitions();
      // Undeclared exception!
      try { 
        defaultPartitionCoalescer_PartitionLocations0.getAllPrefLocs(rDD1);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("scala.collection.mutable.ArrayOps$ofRef$", e);
      }
  }

  @Test(timeout = 4000)
  public void test23()  throws Throwable  {
      DefaultPartitionCoalescer defaultPartitionCoalescer0 = new DefaultPartitionCoalescer(2973.69596019822);
      Partition[] partitionArray0 = new Partition[1];
      Partition partition0 = mock(Partition.class, new ViolatedAssumptionAnswer());
      doReturn("coalesced RDDs with partial locality, large scale (10K partitions)", (String) null).when(partition0).toString();
      doReturn(1371).when(partition0).index();
      partitionArray0[0] = partition0;
      ArrayBuffer<PartitionGroup> arrayBuffer0 = defaultPartitionCoalescer0.groupArr();
      Seq<TaskLocation> seq0 = (Seq<TaskLocation>) mock(Seq.class, new ViolatedAssumptionAnswer());
      doReturn(arrayBuffer0).when(seq0).map(any(scala.Function1.class) , any(scala.collection.generic.CanBuildFrom.class));
      SparkContext sparkContext0 = mock(SparkContext.class, new ViolatedAssumptionAnswer());
      doReturn(seq0).when(sparkContext0).getPreferredLocs(any(org.apache.spark.rdd.RDD.class) , anyInt());
      RDD<PartitionGroup> rDD0 = (RDD<PartitionGroup>) mock(RDD.class, new ViolatedAssumptionAnswer());
      doReturn(sparkContext0).when(rDD0).context();
      doReturn(partitionArray0).when(rDD0).partitions();
      DefaultPartitionCoalescer.PartitionLocations defaultPartitionCoalescer_PartitionLocations0 = defaultPartitionCoalescer0.new PartitionLocations(rDD0);
      defaultPartitionCoalescer_PartitionLocations0.partsWithoutLocs();
      assertTrue(defaultPartitionCoalescer0.noLocality());
  }

  @Test(timeout = 4000)
  public void test24()  throws Throwable  {
      DefaultPartitionCoalescer defaultPartitionCoalescer0 = new DefaultPartitionCoalescer(0.0);
      defaultPartitionCoalescer0.initialHash();
      assertTrue(defaultPartitionCoalescer0.noLocality());
  }

  @Test(timeout = 4000)
  public void test25()  throws Throwable  {
      DefaultPartitionCoalescer defaultPartitionCoalescer0 = new DefaultPartitionCoalescer(0.0);
      double double0 = defaultPartitionCoalescer0.balanceSlack();
      assertEquals(0.0, double0, 0.01);
      assertTrue(defaultPartitionCoalescer0.noLocality());
  }

  @Test(timeout = 4000)
  public void test26()  throws Throwable  {
      DefaultPartitionCoalescer defaultPartitionCoalescer0 = new DefaultPartitionCoalescer(0.0);
      defaultPartitionCoalescer0.getPartitions();
      assertTrue(defaultPartitionCoalescer0.noLocality());
  }

  @Test(timeout = 4000)
  public void test27()  throws Throwable  {
      DefaultPartitionCoalescer defaultPartitionCoalescer0 = new DefaultPartitionCoalescer(0.0);
      // Undeclared exception!
      try { 
        defaultPartitionCoalescer0.currPrefLocs((Partition) null, (RDD<?>) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.spark.rdd.DefaultPartitionCoalescer", e);
      }
  }

  @Test(timeout = 4000)
  public void test28()  throws Throwable  {
      DefaultPartitionCoalescer defaultPartitionCoalescer0 = new DefaultPartitionCoalescer(2973.69596019822);
      Partition[] partitionArray0 = new Partition[1];
      Partition partition0 = mock(Partition.class, new ViolatedAssumptionAnswer());
      doReturn("~Rg'.z&a@(37=)@z").when(partition0).toString();
      doReturn(1371).when(partition0).index();
      partitionArray0[0] = partition0;
      ArrayBuffer<PartitionGroup> arrayBuffer0 = defaultPartitionCoalescer0.groupArr();
      Seq<TaskLocation> seq0 = (Seq<TaskLocation>) mock(Seq.class, new ViolatedAssumptionAnswer());
      doReturn(arrayBuffer0).when(seq0).map(any(scala.Function1.class) , any(scala.collection.generic.CanBuildFrom.class));
      SparkContext sparkContext0 = mock(SparkContext.class, new ViolatedAssumptionAnswer());
      doReturn(seq0).when(sparkContext0).getPreferredLocs(any(org.apache.spark.rdd.RDD.class) , anyInt());
      Partition[] partitionArray1 = new Partition[0];
      RDD<PartitionGroup> rDD0 = (RDD<PartitionGroup>) mock(RDD.class, new ViolatedAssumptionAnswer());
      doReturn(sparkContext0).when(rDD0).context();
      doReturn((Object) partitionArray0, (Object) partitionArray0, (Object) partitionArray1).when(rDD0).partitions();
      PartitionGroup[] partitionGroupArray0 = defaultPartitionCoalescer0.coalesce(484, rDD0);
      assertEquals(1, arrayBuffer0.size0());
      assertEquals(0, partitionGroupArray0.length);
  }

  @Test(timeout = 4000)
  public void test29()  throws Throwable  {
      DefaultPartitionCoalescer defaultPartitionCoalescer0 = new DefaultPartitionCoalescer(0.05);
      Partition[] partitionArray0 = new Partition[2];
      RDD<PartitionGroup> rDD0 = (RDD<PartitionGroup>) mock(RDD.class, new ViolatedAssumptionAnswer());
      doReturn(partitionArray0).when(rDD0).partitions();
      // Undeclared exception!
      try { 
        defaultPartitionCoalescer0.throwBalls(7919, rDD0, 7919, (DefaultPartitionCoalescer.PartitionLocations) null);
        fail("Expecting exception: IndexOutOfBoundsException");
      
      } catch(IndexOutOfBoundsException e) {
         //
         // java.lang.Integer@0000000025
         //
         verifyException("scala.collection.mutable.ResizableArray$class", e);
      }
  }

  @Test(timeout = 4000)
  public void test30()  throws Throwable  {
      DefaultPartitionCoalescer defaultPartitionCoalescer0 = new DefaultPartitionCoalescer(2973.69596019822);
      Partition[] partitionArray0 = new Partition[1];
      Partition partition0 = mock(Partition.class, new ViolatedAssumptionAnswer());
      doReturn("H1%;Y&YJBu5K8%", "H1%;Y&YJBu5K8%", (String) null, (String) null).when(partition0).toString();
      doReturn(1371).when(partition0).index();
      defaultPartitionCoalescer0.noLocality_$eq(false);
      partitionArray0[0] = partition0;
      ArrayBuffer<PartitionGroup> arrayBuffer0 = defaultPartitionCoalescer0.groupArr();
      Seq<TaskLocation> seq0 = (Seq<TaskLocation>) mock(Seq.class, new ViolatedAssumptionAnswer());
      doReturn(arrayBuffer0).when(seq0).map(any(scala.Function1.class) , any(scala.collection.generic.CanBuildFrom.class));
      SparkContext sparkContext0 = mock(SparkContext.class, new ViolatedAssumptionAnswer());
      doReturn(seq0).when(sparkContext0).getPreferredLocs(any(org.apache.spark.rdd.RDD.class) , anyInt());
      RDD<PartitionGroup> rDD0 = (RDD<PartitionGroup>) mock(RDD.class, new ViolatedAssumptionAnswer());
      doReturn(sparkContext0).when(rDD0).context();
      doReturn((Object) partitionArray0, (Object) partitionArray0, (Object) partitionArray0).when(rDD0).partitions();
      defaultPartitionCoalescer0.coalesce(1371, rDD0);
      assertEquals(1, arrayBuffer0.size0());
      
      boolean boolean0 = defaultPartitionCoalescer0.addPartToPGroup(partition0, (PartitionGroup) null);
      assertFalse(boolean0);
  }

  @Test(timeout = 4000)
  public void test31()  throws Throwable  {
      DefaultPartitionCoalescer defaultPartitionCoalescer0 = new DefaultPartitionCoalescer(2973.69596019822);
      // Undeclared exception!
      try { 
        defaultPartitionCoalescer0.addPartToPGroup((Partition) null, (PartitionGroup) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.spark.rdd.DefaultPartitionCoalescer", e);
      }
  }

  @Test(timeout = 4000)
  public void test32()  throws Throwable  {
      DefaultPartitionCoalescer defaultPartitionCoalescer0 = new DefaultPartitionCoalescer(0.05);
      // Undeclared exception!
      try { 
        defaultPartitionCoalescer0.compare((Option<PartitionGroup>) null, (Option<PartitionGroup>) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.spark.rdd.DefaultPartitionCoalescer", e);
      }
  }

  @Test(timeout = 4000)
  public void test33()  throws Throwable  {
      double double0 = DefaultPartitionCoalescer.$lessinit$greater$default$1();
      assertEquals(0.1, double0, 0.01);
  }

  @Test(timeout = 4000)
  public void test34()  throws Throwable  {
      DefaultPartitionCoalescer defaultPartitionCoalescer0 = new DefaultPartitionCoalescer((-1407.612147754901));
      defaultPartitionCoalescer0.getLeastGroupHash("org.apache.spark.rdd.DefaultPartitionCoalescer$PartitionLocations");
      assertTrue(defaultPartitionCoalescer0.noLocality());
  }

  @Test(timeout = 4000)
  public void test35()  throws Throwable  {
      DefaultPartitionCoalescer defaultPartitionCoalescer0 = new DefaultPartitionCoalescer(2973.69596019822);
      Partition[] partitionArray0 = new Partition[1];
      Partition partition0 = mock(Partition.class, new ViolatedAssumptionAnswer());
      doReturn("H1%;Y&YJBu5K8%").when(partition0).toString();
      doReturn(1371).when(partition0).index();
      partitionArray0[0] = partition0;
      ArrayBuffer<PartitionGroup> arrayBuffer0 = defaultPartitionCoalescer0.groupArr();
      Seq<TaskLocation> seq0 = (Seq<TaskLocation>) mock(Seq.class, new ViolatedAssumptionAnswer());
      doReturn(arrayBuffer0).when(seq0).map(any(scala.Function1.class) , any(scala.collection.generic.CanBuildFrom.class));
      SparkContext sparkContext0 = mock(SparkContext.class, new ViolatedAssumptionAnswer());
      doReturn(seq0).when(sparkContext0).getPreferredLocs(any(org.apache.spark.rdd.RDD.class) , anyInt());
      RDD<PartitionGroup> rDD0 = (RDD<PartitionGroup>) mock(RDD.class, new ViolatedAssumptionAnswer());
      doReturn(sparkContext0).when(rDD0).context();
      doReturn((Object) partitionArray0, (Object) partitionArray0, (Object) partitionArray0, (Object) partitionArray0, (Object) null).when(rDD0).partitions();
      // Undeclared exception!
      try { 
        defaultPartitionCoalescer0.coalesce(1, rDD0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.spark.rdd.DefaultPartitionCoalescer$$anonfun$throwBalls$1$$anonfun$apply$mcVI$sp$2", e);
      }
  }
}
