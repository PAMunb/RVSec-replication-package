/*
 * This file was automatically generated by EvoSuite
 * Thu Apr 21 22:09:16 GMT 2022
 */

package org.apache.spark.shuffle.sort;

import org.junit.Test;
import static org.junit.Assert.*;
import static org.evosuite.shaded.org.mockito.Mockito.*;
import static org.evosuite.runtime.EvoAssertions.*;
import java.io.File;
import org.apache.spark.Partitioner;
import org.apache.spark.ShuffleDependency;
import org.apache.spark.SparkConf;
import org.apache.spark.TaskContext;
import org.apache.spark.executor.ShuffleWriteMetrics;
import org.apache.spark.executor.TaskMetrics;
import org.apache.spark.scheduler.HighlyCompressedMapStatus;
import org.apache.spark.serializer.Serializer;
import org.apache.spark.serializer.SerializerInstance;
import org.apache.spark.shuffle.IndexShuffleBlockResolver;
import org.apache.spark.shuffle.sort.BypassMergeSortShuffleHandle;
import org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter;
import org.apache.spark.storage.BlockManager;
import org.apache.spark.storage.BlockManagerId;
import org.apache.spark.storage.DiskBlockManager;
import org.apache.spark.storage.DiskBlockObjectWriter;
import org.apache.spark.storage.TempShuffleBlockId;
import org.evosuite.runtime.EvoRunner;
import org.evosuite.runtime.EvoRunnerParameters;
import org.evosuite.runtime.ViolatedAssumptionAnswer;
import org.junit.runner.RunWith;
import scala.Product2;
import scala.Tuple2;
import scala.collection.Iterator;

@RunWith(EvoRunner.class) @EvoRunnerParameters(mockJVMNonDeterminism = true, useVFS = true, useVNET = true, resetStaticState = true, separateClassLoader = true) 
public class BypassMergeSortShuffleWriter_ESTest extends BypassMergeSortShuffleWriter_ESTest_scaffolding {

  @Test(timeout = 4000)
  public void test0()  throws Throwable  {
      BlockManager blockManager0 = mock(BlockManager.class, new ViolatedAssumptionAnswer());
      IndexShuffleBlockResolver indexShuffleBlockResolver0 = mock(IndexShuffleBlockResolver.class, new ViolatedAssumptionAnswer());
      TaskContext taskContext0 = mock(TaskContext.class, new ViolatedAssumptionAnswer());
      SparkConf sparkConf0 = mock(SparkConf.class, new ViolatedAssumptionAnswer());
      doReturn(false).when(sparkConf0).getBoolean(anyString() , anyBoolean());
      doReturn((long)(-243)).when(sparkConf0).getSizeAsKb(anyString() , anyString());
      BypassMergeSortShuffleWriter<HighlyCompressedMapStatus, HighlyCompressedMapStatus> bypassMergeSortShuffleWriter0 = null;
      try {
        bypassMergeSortShuffleWriter0 = new BypassMergeSortShuffleWriter<HighlyCompressedMapStatus, HighlyCompressedMapStatus>(blockManager0, indexShuffleBlockResolver0, (BypassMergeSortShuffleHandle<HighlyCompressedMapStatus, HighlyCompressedMapStatus>) null, (-243), taskContext0, sparkConf0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter", e);
      }
  }

  @Test(timeout = 4000)
  public void test1()  throws Throwable  {
      BlockManager blockManager0 = mock(BlockManager.class, new ViolatedAssumptionAnswer());
      IndexShuffleBlockResolver indexShuffleBlockResolver0 = mock(IndexShuffleBlockResolver.class, new ViolatedAssumptionAnswer());
      Partitioner partitioner0 = mock(Partitioner.class, new ViolatedAssumptionAnswer());
      doReturn(5277).when(partitioner0).numPartitions();
      Serializer serializer0 = mock(Serializer.class, new ViolatedAssumptionAnswer());
      ShuffleDependency<HighlyCompressedMapStatus, HighlyCompressedMapStatus, HighlyCompressedMapStatus> shuffleDependency0 = (ShuffleDependency<HighlyCompressedMapStatus, HighlyCompressedMapStatus, HighlyCompressedMapStatus>) mock(ShuffleDependency.class, new ViolatedAssumptionAnswer());
      doReturn(partitioner0).when(shuffleDependency0).partitioner();
      doReturn(serializer0).when(shuffleDependency0).serializer();
      doReturn(5277).when(shuffleDependency0).shuffleId();
      BypassMergeSortShuffleHandle<HighlyCompressedMapStatus, HighlyCompressedMapStatus> bypassMergeSortShuffleHandle0 = (BypassMergeSortShuffleHandle<HighlyCompressedMapStatus, HighlyCompressedMapStatus>) mock(BypassMergeSortShuffleHandle.class, new ViolatedAssumptionAnswer());
      doReturn(shuffleDependency0).when(bypassMergeSortShuffleHandle0).dependency();
      ShuffleWriteMetrics shuffleWriteMetrics0 = mock(ShuffleWriteMetrics.class, new ViolatedAssumptionAnswer());
      TaskMetrics taskMetrics0 = mock(TaskMetrics.class, new ViolatedAssumptionAnswer());
      doReturn(shuffleWriteMetrics0).when(taskMetrics0).shuffleWriteMetrics();
      TaskContext taskContext0 = mock(TaskContext.class, new ViolatedAssumptionAnswer());
      doReturn(taskMetrics0).when(taskContext0).taskMetrics();
      SparkConf sparkConf0 = mock(SparkConf.class, new ViolatedAssumptionAnswer());
      doReturn(true).when(sparkConf0).getBoolean(anyString() , anyBoolean());
      doReturn((long)5277).when(sparkConf0).getSizeAsKb(anyString() , anyString());
      BypassMergeSortShuffleWriter<HighlyCompressedMapStatus, HighlyCompressedMapStatus> bypassMergeSortShuffleWriter0 = new BypassMergeSortShuffleWriter<HighlyCompressedMapStatus, HighlyCompressedMapStatus>(blockManager0, indexShuffleBlockResolver0, bypassMergeSortShuffleHandle0, 5277, taskContext0, sparkConf0);
      // Undeclared exception!
      try { 
        bypassMergeSortShuffleWriter0.stop(true);
        fail("Expecting exception: IllegalStateException");
      
      } catch(IllegalStateException e) {
         //
         // Cannot call stop(true) without having called write()
         //
         verifyException("org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter", e);
      }
  }

  @Test(timeout = 4000)
  public void test2()  throws Throwable  {
      Tuple2<TempShuffleBlockId, File> tuple2_0 = (Tuple2<TempShuffleBlockId, File>) mock(Tuple2.class, new ViolatedAssumptionAnswer());
      doReturn((Object) null).when(tuple2_0)._1();
      doReturn((Object) null).when(tuple2_0)._2();
      DiskBlockManager diskBlockManager0 = mock(DiskBlockManager.class, new ViolatedAssumptionAnswer());
      doReturn(tuple2_0).when(diskBlockManager0).createTempShuffleBlock();
      BlockManager blockManager0 = mock(BlockManager.class, new ViolatedAssumptionAnswer());
      doReturn(diskBlockManager0, (DiskBlockManager) null).when(blockManager0).diskBlockManager();
      doReturn((DiskBlockObjectWriter) null).when(blockManager0).getDiskWriter(any(org.apache.spark.storage.BlockId.class) , any(java.io.File.class) , any(org.apache.spark.serializer.SerializerInstance.class) , anyInt() , any(org.apache.spark.executor.ShuffleWriteMetrics.class));
      IndexShuffleBlockResolver indexShuffleBlockResolver0 = mock(IndexShuffleBlockResolver.class, new ViolatedAssumptionAnswer());
      Partitioner partitioner0 = mock(Partitioner.class, new ViolatedAssumptionAnswer());
      doReturn(5277).when(partitioner0).numPartitions();
      SerializerInstance serializerInstance0 = mock(SerializerInstance.class, new ViolatedAssumptionAnswer());
      Serializer serializer0 = mock(Serializer.class, new ViolatedAssumptionAnswer());
      doReturn(serializerInstance0).when(serializer0).newInstance();
      ShuffleDependency<HighlyCompressedMapStatus, HighlyCompressedMapStatus, HighlyCompressedMapStatus> shuffleDependency0 = (ShuffleDependency<HighlyCompressedMapStatus, HighlyCompressedMapStatus, HighlyCompressedMapStatus>) mock(ShuffleDependency.class, new ViolatedAssumptionAnswer());
      doReturn(partitioner0).when(shuffleDependency0).partitioner();
      doReturn(serializer0).when(shuffleDependency0).serializer();
      doReturn(0).when(shuffleDependency0).shuffleId();
      BypassMergeSortShuffleHandle<HighlyCompressedMapStatus, HighlyCompressedMapStatus> bypassMergeSortShuffleHandle0 = (BypassMergeSortShuffleHandle<HighlyCompressedMapStatus, HighlyCompressedMapStatus>) mock(BypassMergeSortShuffleHandle.class, new ViolatedAssumptionAnswer());
      doReturn(shuffleDependency0).when(bypassMergeSortShuffleHandle0).dependency();
      ShuffleWriteMetrics shuffleWriteMetrics0 = mock(ShuffleWriteMetrics.class, new ViolatedAssumptionAnswer());
      TaskMetrics taskMetrics0 = mock(TaskMetrics.class, new ViolatedAssumptionAnswer());
      doReturn(shuffleWriteMetrics0).when(taskMetrics0).shuffleWriteMetrics();
      TaskContext taskContext0 = mock(TaskContext.class, new ViolatedAssumptionAnswer());
      doReturn(taskMetrics0).when(taskContext0).taskMetrics();
      boolean boolean0 = false;
      SparkConf sparkConf0 = mock(SparkConf.class, new ViolatedAssumptionAnswer());
      doReturn(false).when(sparkConf0).getBoolean(anyString() , anyBoolean());
      doReturn((-2512L)).when(sparkConf0).getSizeAsKb(anyString() , anyString());
      BypassMergeSortShuffleWriter<HighlyCompressedMapStatus, HighlyCompressedMapStatus> bypassMergeSortShuffleWriter0 = new BypassMergeSortShuffleWriter<HighlyCompressedMapStatus, HighlyCompressedMapStatus>(blockManager0, indexShuffleBlockResolver0, bypassMergeSortShuffleHandle0, 11, taskContext0, sparkConf0);
      bypassMergeSortShuffleWriter0.stop(false);
      bypassMergeSortShuffleWriter0.stop(false);
      bypassMergeSortShuffleWriter0.stop(false);
      Iterator<Product2<HighlyCompressedMapStatus, HighlyCompressedMapStatus>> iterator0 = (Iterator<Product2<HighlyCompressedMapStatus, HighlyCompressedMapStatus>>) mock(Iterator.class, new ViolatedAssumptionAnswer());
      doReturn(true).when(iterator0).hasNext();
      // Undeclared exception!
      try { 
        bypassMergeSortShuffleWriter0.write(iterator0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter", e);
      }
  }

  @Test(timeout = 4000)
  public void test3()  throws Throwable  {
      BlockManager blockManager0 = mock(BlockManager.class, new ViolatedAssumptionAnswer());
      doReturn((BlockManagerId) null).when(blockManager0).shuffleServerId();
      IndexShuffleBlockResolver indexShuffleBlockResolver0 = mock(IndexShuffleBlockResolver.class, new ViolatedAssumptionAnswer());
      Partitioner partitioner0 = mock(Partitioner.class, new ViolatedAssumptionAnswer());
      doReturn(5277).when(partitioner0).numPartitions();
      Serializer serializer0 = mock(Serializer.class, new ViolatedAssumptionAnswer());
      ShuffleDependency<HighlyCompressedMapStatus, HighlyCompressedMapStatus, HighlyCompressedMapStatus> shuffleDependency0 = (ShuffleDependency<HighlyCompressedMapStatus, HighlyCompressedMapStatus, HighlyCompressedMapStatus>) mock(ShuffleDependency.class, new ViolatedAssumptionAnswer());
      doReturn(partitioner0).when(shuffleDependency0).partitioner();
      doReturn(serializer0).when(shuffleDependency0).serializer();
      doReturn(0).when(shuffleDependency0).shuffleId();
      BypassMergeSortShuffleHandle<HighlyCompressedMapStatus, HighlyCompressedMapStatus> bypassMergeSortShuffleHandle0 = (BypassMergeSortShuffleHandle<HighlyCompressedMapStatus, HighlyCompressedMapStatus>) mock(BypassMergeSortShuffleHandle.class, new ViolatedAssumptionAnswer());
      doReturn(shuffleDependency0).when(bypassMergeSortShuffleHandle0).dependency();
      ShuffleWriteMetrics shuffleWriteMetrics0 = mock(ShuffleWriteMetrics.class, new ViolatedAssumptionAnswer());
      TaskMetrics taskMetrics0 = mock(TaskMetrics.class, new ViolatedAssumptionAnswer());
      doReturn(shuffleWriteMetrics0).when(taskMetrics0).shuffleWriteMetrics();
      TaskContext taskContext0 = mock(TaskContext.class, new ViolatedAssumptionAnswer());
      doReturn(taskMetrics0).when(taskContext0).taskMetrics();
      SparkConf sparkConf0 = mock(SparkConf.class, new ViolatedAssumptionAnswer());
      doReturn(false).when(sparkConf0).getBoolean(anyString() , anyBoolean());
      doReturn((-2512L)).when(sparkConf0).getSizeAsKb(anyString() , anyString());
      BypassMergeSortShuffleWriter<HighlyCompressedMapStatus, HighlyCompressedMapStatus> bypassMergeSortShuffleWriter0 = new BypassMergeSortShuffleWriter<HighlyCompressedMapStatus, HighlyCompressedMapStatus>(blockManager0, indexShuffleBlockResolver0, bypassMergeSortShuffleHandle0, 11, taskContext0, sparkConf0);
      Iterator<Product2<HighlyCompressedMapStatus, HighlyCompressedMapStatus>> iterator0 = (Iterator<Product2<HighlyCompressedMapStatus, HighlyCompressedMapStatus>>) mock(Iterator.class, new ViolatedAssumptionAnswer());
      doReturn(false).when(iterator0).hasNext();
      // Undeclared exception!
      try { 
        bypassMergeSortShuffleWriter0.write(iterator0);
        fail("Expecting exception: NoClassDefFoundError");
      
      } catch(NoClassDefFoundError e) {
         //
         // Could not initialize class org.apache.spark.internal.config.package$
         //
         verifyException("org.apache.spark.scheduler.HighlyCompressedMapStatus$$anonfun$1", e);
      }
  }

  @Test(timeout = 4000)
  public void test4()  throws Throwable  {
      Tuple2<TempShuffleBlockId, File> tuple2_0 = (Tuple2<TempShuffleBlockId, File>) mock(Tuple2.class, new ViolatedAssumptionAnswer());
      DiskBlockManager diskBlockManager0 = mock(DiskBlockManager.class, new ViolatedAssumptionAnswer());
      BlockManager blockManager0 = mock(BlockManager.class, new ViolatedAssumptionAnswer());
      doReturn((DiskBlockManager) null).when(blockManager0).diskBlockManager();
      IndexShuffleBlockResolver indexShuffleBlockResolver0 = mock(IndexShuffleBlockResolver.class, new ViolatedAssumptionAnswer());
      Partitioner partitioner0 = mock(Partitioner.class, new ViolatedAssumptionAnswer());
      doReturn(5277).when(partitioner0).numPartitions();
      SerializerInstance serializerInstance0 = mock(SerializerInstance.class, new ViolatedAssumptionAnswer());
      Serializer serializer0 = mock(Serializer.class, new ViolatedAssumptionAnswer());
      doReturn((SerializerInstance) null).when(serializer0).newInstance();
      ShuffleDependency<HighlyCompressedMapStatus, HighlyCompressedMapStatus, HighlyCompressedMapStatus> shuffleDependency0 = (ShuffleDependency<HighlyCompressedMapStatus, HighlyCompressedMapStatus, HighlyCompressedMapStatus>) mock(ShuffleDependency.class, new ViolatedAssumptionAnswer());
      doReturn(partitioner0).when(shuffleDependency0).partitioner();
      doReturn(serializer0).when(shuffleDependency0).serializer();
      doReturn(0).when(shuffleDependency0).shuffleId();
      BypassMergeSortShuffleHandle<HighlyCompressedMapStatus, HighlyCompressedMapStatus> bypassMergeSortShuffleHandle0 = (BypassMergeSortShuffleHandle<HighlyCompressedMapStatus, HighlyCompressedMapStatus>) mock(BypassMergeSortShuffleHandle.class, new ViolatedAssumptionAnswer());
      doReturn(shuffleDependency0).when(bypassMergeSortShuffleHandle0).dependency();
      ShuffleWriteMetrics shuffleWriteMetrics0 = mock(ShuffleWriteMetrics.class, new ViolatedAssumptionAnswer());
      TaskMetrics taskMetrics0 = mock(TaskMetrics.class, new ViolatedAssumptionAnswer());
      doReturn(shuffleWriteMetrics0).when(taskMetrics0).shuffleWriteMetrics();
      TaskContext taskContext0 = mock(TaskContext.class, new ViolatedAssumptionAnswer());
      doReturn(taskMetrics0).when(taskContext0).taskMetrics();
      SparkConf sparkConf0 = mock(SparkConf.class, new ViolatedAssumptionAnswer());
      doReturn(false).when(sparkConf0).getBoolean(anyString() , anyBoolean());
      doReturn((-2512L)).when(sparkConf0).getSizeAsKb(anyString() , anyString());
      BypassMergeSortShuffleWriter<HighlyCompressedMapStatus, HighlyCompressedMapStatus> bypassMergeSortShuffleWriter0 = new BypassMergeSortShuffleWriter<HighlyCompressedMapStatus, HighlyCompressedMapStatus>(blockManager0, indexShuffleBlockResolver0, bypassMergeSortShuffleHandle0, 11, taskContext0, sparkConf0);
      Iterator<Product2<HighlyCompressedMapStatus, HighlyCompressedMapStatus>> iterator0 = (Iterator<Product2<HighlyCompressedMapStatus, HighlyCompressedMapStatus>>) mock(Iterator.class, new ViolatedAssumptionAnswer());
      doReturn(true).when(iterator0).hasNext();
      // Undeclared exception!
      try { 
        bypassMergeSortShuffleWriter0.write(iterator0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter", e);
      }
  }

  @Test(timeout = 4000)
  public void test5()  throws Throwable  {
      BlockManager blockManager0 = mock(BlockManager.class, new ViolatedAssumptionAnswer());
      IndexShuffleBlockResolver indexShuffleBlockResolver0 = mock(IndexShuffleBlockResolver.class, new ViolatedAssumptionAnswer());
      Partitioner partitioner0 = mock(Partitioner.class, new ViolatedAssumptionAnswer());
      doReturn(5277).when(partitioner0).numPartitions();
      Serializer serializer0 = mock(Serializer.class, new ViolatedAssumptionAnswer());
      ShuffleDependency<HighlyCompressedMapStatus, HighlyCompressedMapStatus, HighlyCompressedMapStatus> shuffleDependency0 = (ShuffleDependency<HighlyCompressedMapStatus, HighlyCompressedMapStatus, HighlyCompressedMapStatus>) mock(ShuffleDependency.class, new ViolatedAssumptionAnswer());
      doReturn(partitioner0).when(shuffleDependency0).partitioner();
      doReturn(serializer0).when(shuffleDependency0).serializer();
      doReturn(0).when(shuffleDependency0).shuffleId();
      BypassMergeSortShuffleHandle<HighlyCompressedMapStatus, HighlyCompressedMapStatus> bypassMergeSortShuffleHandle0 = (BypassMergeSortShuffleHandle<HighlyCompressedMapStatus, HighlyCompressedMapStatus>) mock(BypassMergeSortShuffleHandle.class, new ViolatedAssumptionAnswer());
      doReturn(shuffleDependency0).when(bypassMergeSortShuffleHandle0).dependency();
      ShuffleWriteMetrics shuffleWriteMetrics0 = mock(ShuffleWriteMetrics.class, new ViolatedAssumptionAnswer());
      TaskMetrics taskMetrics0 = mock(TaskMetrics.class, new ViolatedAssumptionAnswer());
      doReturn(shuffleWriteMetrics0).when(taskMetrics0).shuffleWriteMetrics();
      TaskContext taskContext0 = mock(TaskContext.class, new ViolatedAssumptionAnswer());
      doReturn(taskMetrics0).when(taskContext0).taskMetrics();
      SparkConf sparkConf0 = mock(SparkConf.class, new ViolatedAssumptionAnswer());
      doReturn(false).when(sparkConf0).getBoolean(anyString() , anyBoolean());
      doReturn((-2512L)).when(sparkConf0).getSizeAsKb(anyString() , anyString());
      BypassMergeSortShuffleWriter<HighlyCompressedMapStatus, HighlyCompressedMapStatus> bypassMergeSortShuffleWriter0 = new BypassMergeSortShuffleWriter<HighlyCompressedMapStatus, HighlyCompressedMapStatus>(blockManager0, indexShuffleBlockResolver0, bypassMergeSortShuffleHandle0, 11, taskContext0, sparkConf0);
      long[] longArray0 = bypassMergeSortShuffleWriter0.getPartitionLengths();
      assertNull(longArray0);
  }
}
