/*
 * This file was automatically generated by EvoSuite
 * Thu Apr 21 21:58:26 GMT 2022
 */

package org.apache.spark.scheduler;

import org.junit.Test;
import static org.junit.Assert.*;
import org.apache.spark.TaskEndReason;
import org.apache.spark.scheduler.CompletionEvent;
import org.apache.spark.scheduler.Task;
import org.apache.spark.scheduler.TaskInfo;
import org.apache.spark.util.AccumulatorV2;
import org.evosuite.runtime.EvoRunner;
import org.evosuite.runtime.EvoRunnerParameters;
import org.junit.runner.RunWith;
import scala.Function1;
import scala.Option;
import scala.Tuple5;
import scala.collection.Seq;

@RunWith(EvoRunner.class) @EvoRunnerParameters(mockJVMNonDeterminism = true, useVFS = true, useVNET = true, resetStaticState = true, separateClassLoader = true) 
public class CompletionEvent_ESTest extends CompletionEvent_ESTest_scaffolding {

  @Test(timeout = 4000)
  public void test0()  throws Throwable  {
      Option<Tuple5<Task<Object>, TaskEndReason, Object, Seq<AccumulatorV2<?, ?>>, TaskInfo>> option0 = CompletionEvent.unapply((CompletionEvent) null);
      assertNotNull(option0);
  }

  @Test(timeout = 4000)
  public void test1()  throws Throwable  {
      Function1<Tuple5<Task<?>, TaskEndReason, Object, Seq<AccumulatorV2<?, ?>>, TaskInfo>, CompletionEvent> function1_0 = CompletionEvent.tupled();
      assertNotNull(function1_0);
  }

  @Test(timeout = 4000)
  public void test2()  throws Throwable  {
      Function1<Task<?>, Function1<TaskEndReason, Function1<Object, Function1<Seq<AccumulatorV2<?, ?>>, Function1<TaskInfo, CompletionEvent>>>>> function1_0 = CompletionEvent.curried();
      assertNotNull(function1_0);
  }
}
